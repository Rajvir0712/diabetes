# -*- coding: utf-8 -*-
"""project_diabetes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DxpsVvVm5tU8p1fCJI_uAXnzApOTaLgn

#Importing Data and dataframe creation
"""

!pip install bayesian-optimization

#Importing the libraries here
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math
import seaborn as sns
from sklearn.model_selection import KFold,train_test_split,GridSearchCV
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
from scipy import stats
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import StandardScaler
from bayes_opt import BayesianOptimization
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RandomizedSearchCV

#Loading the data
url = 'https://raw.githubusercontent.com/Rajvir0712/diabetes/main/diabetes.csv'
database = pd.read_csv(url)

database.head()
database["Outcome"].value_counts()

"""#Preprocessing Step - For preprocessing we will do 2 things, outlier rejections and filling missing values and Standardization if necessary and doing feature selection based on correlation."""

database.isnull().values.any()

"""Here we can see that our database has no missing values"""

threshold = 3

# Calculate Z-scores for all columns except 'Outcome'
features = database.drop('Outcome', axis=1)
z_scores = stats.zscore(features)

# Calculate absolute Z-scores
abs_z_scores = np.abs(z_scores)

# Create a boolean mask where True indicates the rows that do not contain outliers
filtered_entries = (abs_z_scores < threshold).all(axis=1)

# Apply the mask to the DataFrame to get a cleaned dataset without outliers
cleaned_data = database[filtered_entries]

database = cleaned_data
cleaned_data.head()
cleaned_data['Outcome'].value_counts()

def reject_outliers(df, m=1.5):
    outlier_indices = []
    for col in df.select_dtypes(include=['float64', 'int64']).columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - m * IQR
        upper_bound = Q3 + m * IQR
        outlier_col = df[(df[col] < lower_bound) | (df[col] > upper_bound)].index
        outlier_indices.extend(outlier_col)
    outlier_indices = list(set(outlier_indices))  # Remove duplicates
    df_no_outliers = df.drop(index=outlier_indices)
    return df_no_outliers

# Applying the function to the DataFrame without the 'Outcome' column
features = database.drop('Outcome', axis=1)
cleaned_data_no_outliers = reject_outliers(features)

# Add the 'Outcome' column back to the cleaned data
cleaned_data_no_outliers['Outcome'] = database.loc[cleaned_data_no_outliers.index, 'Outcome']
database = cleaned_data_no_outliers

database['Outcome'].value_counts()

# Calculating the correlation matrix for the dataset after outlier rejection using the IQR method
corr_matrix_iqr_cleaned = cleaned_data.corr()

# Visualizing the correlation matrix using a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix_iqr_cleaned, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix for Data Cleaned with IQR Method')
plt.show()

"""We did outlier rejection using Z-score. You can create features and their graphs to show outlier while doing data vsualization. Z-score with value less than -3 or more than +3 is called outlier here."""

X = database.drop('Outcome', axis=1)  # Features
y = database['Outcome']
scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)

"""#Feature based model selection based on correlation"""

# Calculate correlation matrix with respect to the target variable 'Outcome'
correlation_matrix = database.corr()['Outcome'].sort_values()

# Select features with correlation above the threshold
threshold = 0.2
features_above_threshold = correlation_matrix[abs(correlation_matrix) >= threshold].index.tolist()

# Remove the target variable 'Outcome' from the features list if it's included
features_above_threshold.remove('Outcome')

# Create a new dataframe with the selected features plus the target variable
selected_data = database[features_above_threshold + ['Outcome']]

# Now `selected_data` contains only the features with a correlation above 0.2 with the target variable
print("Selected features based on correlation threshold:", features_above_threshold)
print(selected_data.head())

database = selected_data

"""The paper said standardization reduces accuracy, still if you guys want I can do it. I left the above cell empty for that.

#K FOLD AND SPLITTING DATA
"""

# Assuming 'data' is your DataFrame and 'Outcome' is the target variable.
X = database.drop('Outcome', axis=1)  # Features
y = database['Outcome']  # Target variable

# First, split the data into a training set and a hold-out test set
X_train_full, X_test, y_train_full, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Define the KFold cross-validator
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Placeholder for the model's validation scores
validation_scores = []

# Initialize the XGBoost classifier
model = XGBClassifier(random_state=42)

# Iterate over each fold
for train_index, val_index in kf.split(X_train_full):
    # Split the data into k-fold train and validation sets
    X_train, X_val = X_train_full.iloc[train_index], X_train_full.iloc[val_index]
    y_train, y_val = y_train_full.iloc[train_index], y_train_full.iloc[val_index]

    # Fit the model
    model.fit(X_train, y_train)

    # Predict on the validation set
    y_val_pred = model.predict(X_val)

    # Evaluate the model
    val_score = accuracy_score(y_val, y_val_pred)
    validation_scores.append(val_score)

    # Optionally, you can print out the validation score for each fold
    print(f'Validation score for fold: {val_score}')

# After cross-validation, you might want to fit the model on the entire training set
# and evaluate it on the hold-out test set
model.fit(X_train_full, y_train_full)
y_test_pred = model.predict(X_test)
test_score = accuracy_score(y_test, y_test_pred)

print(f'Test score: {test_score}')

# The validation_scores list contains the validation accuracy for each fold
print(f'Average validation score: {sum(validation_scores) / len(validation_scores)}')

#More robust model is Threshold/z_score model. Test Score and Average score are almost equal.

"""#Training, Grid search (Hyper-parameter tunning using Bayesian Search, Randomised Search and Grid Search), fitting and testing"""

#For Standardization
X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_standardized, y, test_size=0.2, random_state=42)

model = XGBClassifier(random_state=42)
param_grid = {
    'min_child_weight': [1, 5, 10],
    'gamma': [0.5, 1, 1.5, 2, 5],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'max_depth': [3, 4, 5],
    'learning_rate': [0.1,0.01,0.05]
}
grid_search = GridSearchCV(model, param_grid, scoring='roc_auc', n_jobs=-1, cv=3, verbose=3)

# Fit the grid search to the data
grid_search.fit(X_train_1, y_train_1)

# Print the best parameters and best score
print("Best parameters found: ", grid_search.best_params_)
print("Best AUC found: ", grid_search.best_score_)

# Predict on the test data with the best parameters
best_model = grid_search.best_estimator_
y_pred_prob = best_model.predict_proba(X_test_1)[:, 1]

# Calculate and print the AUC score for the best model
auc_score = roc_auc_score(y_test_1, y_pred_prob)
print(f"Test AUC Score with best parameters: {auc_score}")

# Define the function to optimize
def xgb_evaluate(min_child_weight, gamma, subsample, colsample_bytree, max_depth, learning_rate):
    params = {
        'min_child_weight': int(min_child_weight),
        'gamma': gamma,
        'subsample': subsample,
        'colsample_bytree': colsample_bytree,
        'max_depth': int(max_depth),
        'learning_rate': learning_rate,
        'n_estimators': 100,
        'objective': 'binary:logistic'
    }
    xgb = XGBClassifier(**params, random_state=42)
    scores = cross_val_score(xgb, X_train_1, y_train_1, scoring='roc_auc', cv=3)
    return scores.mean()

# Define ranges for the hyperparameters
param_bounds = {
    'min_child_weight': (1, 10),
    'gamma': (0.5, 5),
    'subsample': (0.6, 1.0),
    'colsample_bytree': (0.6, 1.0),
    'max_depth': (1,10),
    'learning_rate': (0.01, 0.1)
}

# Initialize Bayesian Optimization
optimizer = BayesianOptimization(f=xgb_evaluate, pbounds=param_bounds, random_state=42)

# Perform optimization
optimizer.maximize(init_points=10, n_iter=100)

# Results
print("Best parameters found: ", optimizer.max['params'])

best_params = optimizer.max['params']
# Convert any float hyperparameters to integers (as required by XGBClassifier)
best_params['max_depth'] = int(best_params['max_depth'])
best_params['min_child_weight'] = int(best_params['min_child_weight'])

# Create and train the model with the best parameters
best_model = XGBClassifier(**best_params, random_state=42)

best_model.fit(X_train_1, y_train_1)

# Predict probabilities on the test set
y_pred_proba = best_model.predict_proba(X_test_1)[:, 1]

# Calculate AUC score
auc_score = roc_auc_score(y_test_1, y_pred_proba)
print(f"Test AUC Score with best parameters: {auc_score}")

# Define the hyperparameter space
param_distributions = {
    'min_child_weight': [1, 5, 10],
    'gamma': [0.5, 1, 1.5, 2, 5],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'max_depth': [3, 4, 5],
    'learning_rate': [0.1, 0.01, 0.05]
}

# Create the model
model = XGBClassifier(random_state=42)

# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(
    model,
    param_distributions=param_distributions,
    n_iter=100,  # Number of parameter settings sampled
    scoring='roc_auc',
    n_jobs=-1,
    cv=3,
    random_state=42,
    verbose=3
)

# Fit the random search to the data
random_search.fit(X_train_1, y_train_1)

# Print the best parameters and best score
print("Best parameters found: ", random_search.best_params_)
print("Best AUC found: ", random_search.best_score_)

# Predict on the test data with the best parameters
best_model = random_search.best_estimator_
y_pred_prob = best_model.predict_proba(X_test_1)[:, 1]

# Calculate and print the AUC score for the best model
auc_score = roc_auc_score(y_test_1, y_pred_prob)
print(f"Test AUC Score with best parameters: {auc_score}")

"""###Inferences###"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import KFold, train_test_split, GridSearchCV
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc, precision_score, recall_score, f1_score
from scipy import stats
from sklearn.preprocessing import StandardScaler

# Evaluating the model on the test set
model.fit(X_train_full, y_train_full)
y_test_pred = model.predict(X_test_1)
test_score = accuracy_score(y_test_1, y_test_pred)
print(f'Test score: {test_score}')
print(f'Average validation score: {sum(validation_scores) / len(validation_scores)}')

# Confusion matrix and additional metrics
conf_matrix = confusion_matrix(y_test, y_test_pred)

# Print the classification report
print("Classification Report:")
print(classification_report(y_test, y_test_pred))

# Plotting the confusion matrix with additional details
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.text(0.5, -0.15, f'Precision: {precision_score(y_test, y_test_pred):.2f}', ha='center', va='center', transform=plt.gca().transAxes)
plt.text(0.5, -0.2, f'Recall: {recall_score(y_test, y_test_pred):.2f}', ha='center', va='center', transform=plt.gca().transAxes)
plt.text(0.5, -0.25, f'F1-Score: {f1_score(y_test, y_test_pred):.2f}', ha='center', va='center', transform=plt.gca().transAxes)
plt.title('Confusion Matrix with Metrics')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()